from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.storage import LocalFileStore
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
from langchain.memory import ConversationBufferMemory
import streamlit as st
import os


class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message = ""  # ë§¤ë²ˆ ì´ˆê¸°í™”
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


@st.cache_data(show_spinner="Embedding...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./{file.name}"
    # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì €ì¥ ëª¨ë“œ ê²°ì •
    if file.name.endswith(".txt"):
        with open(file_path, "w", encoding="utf-8") as f:
            # bytes â†’ str ë³€í™˜ í•„ìš”
            if isinstance(file_content, bytes):
                file_content = file_content.decode("utf-8")
            f.write(file_content)
    else:
        with open(file_path, "wb") as f:
            f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(splitter)
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings,
        cache_dir
    )
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

def save_message(message, role):
    st.session_state.messages.append({"role": role, "message": message})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state.messages:
        send_message(
            message["message"], 
            message["role"], 
            save=False
        )
        
def format_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])

st.set_page_config(page_title="DocGPT", page_icon=":book:")
st.title("DocGPT")
st.markdown(
    """
    Welcome!
    """
)

if "messages" not in st.session_state:
    st.session_state.messages = []

# ConversationBufferMemory ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="history", return_messages=True)

memory = st.session_state.memory

# ì‚¬ì´ë“œë°”ì— API Key ì…ë ¥ ë° Github ë§í¬ ì¶”ê°€
with st.sidebar:
    st.markdown("[ğŸ”— Github Repo](https://github.com/hughqlee/fullstack_gpt_challenge)")  # ì‹¤ì œ ì£¼ì†Œë¡œ ë³€ê²½ í•„ìš”
    openai_api_key = st.text_input("OpenAI API Key", value=os.environ.get("OPENAI_API_KEY"), type="password")
    file = st.file_uploader(
        "Upload a file", 
        type=["pdf", "docx", "txt"]
    )

# API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šìœ¼ë©´ ì§„í–‰ ë¶ˆê°€ ì•ˆë‚´
if not openai_api_key:
    st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

if file:
    retriever = embed_file(file)
    send_message("I'm ready!", "ai", save=False)
    paint_history()
    
    # ChatCallbackHandler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chat_handler = ChatCallbackHandler()
    
    # RAG íŒŒì´í”„ë¼ì¸ ì •ì˜ (retrieverê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œì ì—ì„œ)
    map_prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up: {context}"),
        ("user", "Question:\n{question}"),
    ])
    
    map_llm = ChatOpenAI(
        temperature=0.1,
        openai_api_key=openai_api_key,
    )
    
    def map_and_prepare(inputs):
        question = inputs["question"]
        docs = retriever.get_relevant_documents(question)
        snippets = []
        for doc in docs:
            out = map_prompt | map_llm
            resp = out.invoke({"context": doc.page_content, "question": question})
            text = resp.content.strip()
            if text:
                snippets.append(text)
        combined_context = "\n\n".join(snippets)
        return {"context": combined_context, "input": question, "history": inputs["history"]}
    
    map_reduce_step = RunnablePassthrough.assign(history=lambda _: memory.load_memory_variables({})['history'])
    map_reduce_step = map_reduce_step | RunnableLambda(map_and_prepare)
    
    final_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ê³¼ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
         "ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì´ì „ ì§ˆë¬¸ë“¤ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ì„¸ìš”. "
         "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ê±°ë‚˜ ëª¨ë¥´ëŠ” ê²ƒì€ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”."),
        ("assistant", "ë¬¸ì„œ ë‚´ìš©:\n{context}"),
        MessagesPlaceholder(variable_name="history"),
        ("user", "{input}"),
    ])
    
    final_llm = ChatOpenAI(
        temperature=0.1,
        streaming=True,
        callbacks=[chat_handler],
        openai_api_key=openai_api_key,
    )
    
    message = st.chat_input("Ask me anything about the file")
    if message:
        send_message(message, "user")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ ë””ë²„ê¹…
        current_memory = memory.load_memory_variables({})
        st.sidebar.write("ğŸ§  Memory Debug:")
        st.sidebar.write(f"Current history length: {len(current_memory.get('history', []))}")
        if current_memory.get('history'):
            st.sidebar.write("Recent messages:")
            for i, msg in enumerate(current_memory['history'][-3:]):  # ìµœê·¼ 3ê°œë§Œ í‘œì‹œ
                st.sidebar.write(f"{i}: {type(msg)} - {str(msg)[:50]}")
        
        chain = map_reduce_step | final_prompt | final_llm
        with st.chat_message("ai"):
            response = chain.invoke({"question": message})
            
            # ì‘ë‹µ ë””ë²„ê¹…
            st.sidebar.write(f"Response type: {type(response)}")
            st.sidebar.write(f"Response content: {str(response)[:100]}")
            
            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
                
            memory.save_context(
                {"input": message},
                {"output": response_text}
            )
            
            # ì €ì¥ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            updated_memory = memory.load_memory_variables({})
            st.sidebar.write(f"After save - history length: {len(updated_memory.get('history', []))}")
else:
    st.session_state.messages = []
        